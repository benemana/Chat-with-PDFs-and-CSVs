{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZzUh42mgM5r"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai tiktoken chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "eQnMD32Uhn9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (SystemMessage, HumanMessage, AIMessage)\n",
        "from langchain.schema import document\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Pinecone, Chroma\n",
        "from langchain.chains import ConversationalRetrievalChain, RetrievalQAWithSourcesChain, RetrievalQA, OpenAIModerationChain, SequentialChain, LLMChain, SimpleSequentialChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "9EJls3o7iEoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import CSVLoader, PyPDFLoader, PyPDFDirectoryLoader, DirectoryLoader, UnstructuredFileLoader"
      ],
      "metadata": {
        "id": "SRF8cK6Uk0DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    model_name='gpt-3.5-turbo',\n",
        "    temperature=0,\n",
        "    openai_api_key = OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "gHbnJaadiY29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Document loading"
      ],
      "metadata": {
        "id": "P-oLSuRZ_J-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured unstructured[pdf]"
      ],
      "metadata": {
        "id": "JlA6rJVv-_Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import tiktoken\n",
        "\n",
        "def rag(directory):\n",
        "  loader = DirectoryLoader(directory, loader_cls=UnstructuredFileLoader)\n",
        "\n",
        "  input_docs = loader.load()\n",
        "  print(str(len(input_docs)) + \" documents loaded\")\n",
        "\n",
        "  # Create function to count tokens\n",
        "  tokenizer = tiktoken.get_encoding('cl100k_base')\n",
        "\n",
        "  def length_function(text: str) -> int:\n",
        "      return len(tokenizer.encode(text))\n",
        "\n",
        "  # Define the splitter\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      length_function=length_function, #usa i token e non i caratteri come unità di splitting\n",
        "      chunk_size=200, #numero massimo di unità in ogni chunk\n",
        "      chunk_overlap=0\n",
        "  )\n",
        "\n",
        "  # Split documents in chunks\n",
        "  doc_chunks = text_splitter.split_documents(input_docs)\n",
        "  print(\"Input Data - Now you have {0} number of chunks.\".format(len(doc_chunks)))\n",
        "\n",
        "  # Define embedding model and vector DB\n",
        "  embed_model = OpenAIEmbeddings(\n",
        "      openai_api_key=OPENAI_API_KEY\n",
        "    )\n",
        "\n",
        "  vDB = Chroma.from_documents(doc_chunks, embed_model)\n",
        "\n",
        "  retriever = vDB.as_retriever()\n",
        "  retriever.search_kwargs = {'k': 20}\n",
        "\n",
        "\n",
        "  # Define memory\n",
        "  memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    input_key='question',\n",
        "    output_key='answer'\n",
        "  )\n",
        "\n",
        "  chatbot = ConversationalRetrievalChain.from_llm(\n",
        "      llm=llm,\n",
        "      retriever=retriever,\n",
        "      memory=memory\n",
        "  )\n",
        "\n",
        "\n",
        "  return chatbot"
      ],
      "metadata": {
        "id": "4Y9Ayy6CLIem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot = rag('/content/documents')"
      ],
      "metadata": {
        "id": "KzpmpIXAoZGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'A quanto ammontano le risorse finanziarie complessivamente messe a disposizione dalla Camera di Commercio?'\n",
        "response = chatbot(\n",
        "    {\"question\":question}\n",
        ")\n",
        "\n",
        "print(\"Question: \" + question)\n",
        "print(\"Answer: \" + response['answer'])\n",
        "#print(\"Sources: \" + response['sources'])\n",
        "\n",
        "chat_history = response['chat_history']"
      ],
      "metadata": {
        "id": "BKNIzQucZxDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Quali sono i requisiti per partecipare al bando?\"\"\"\n",
        "response = chatbot(\n",
        "    {\"question\":question},\n",
        "    return_only_outputs=True\n",
        ")\n",
        "\n",
        "print(\"Question: \" + question)\n",
        "print(\"Answer: \" + response['answer'])"
      ],
      "metadata": {
        "id": "W8mvMQQJF5xe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}